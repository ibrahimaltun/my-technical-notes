
##### 100GB boyuta sahip bir veriyi nasil verimli bir sekilde nasil isleyebiliriz?


**Gerekli Executor Core SayÄ±sÄ±**

- Sparkâ€™ta varsayÄ±lan bir partition boyutu: **128MB**
- 100GB = 102400MB â†’ 102400 / 128 = **800 partition**
- Yani **800 executor core** gerekir (her core bir partition iÅŸler)

**Gerekli Executor SayÄ±sÄ±**

- Genelde bir executorâ€™da **2â€“5 core** olmasÄ± Ã¶nerilir
- Diyelim ki her executorâ€™da 4 core var â†’ 800 / 4 = **200 executor**

**Executor BelleÄŸi**

- Her core iÃ§in Ã¶nerilen bellek: **4 Ã— 128MB = 512MB**
- 4 coreâ€™lu bir executor iÃ§in: 4 Ã— 512MB = **2GB**
- 200 executor Ã— 2GB = **400GB toplam bellek**

**ğŸ•’ Ä°ÅŸlem SÃ¼resi**

- TÃ¼m gÃ¶revler paralel Ã§alÄ±ÅŸÄ±rsa, her biri 5 dakikada bitiyorsa â†’ **100GB veri 5 dakikada iÅŸlenir!**

**Driver BelleÄŸi**

- EÄŸer `df.collect()` yapÄ±yorsan â†’ **100GB driver belleÄŸi** gerekebilir
- Sadece Ã§Ä±ktÄ±yÄ± diske yazÄ±yorsan â†’ **4GB yeterli** (2 Ã— executor belleÄŸi)